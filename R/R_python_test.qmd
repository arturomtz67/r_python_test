---
title: "R and python test"
date: today

execute:
  eval: true # by default is true, runs the code in each chunck 
  echo: true
  warning: false
  message: false
  
format: 
  html:
    code-fold: true # sets all code chunks are collapsed by default
    code-summary: "Show the code"
    toc: true # shows/includes an automatically generated table of contents 
    toc-location: left
    number-sections: true
 
editor: visual

fig-cap-location: margin
---

# Introduction

This document is a test/exercise to run <b>R</b> and <b>Python </b> code in the <b>same</b> Quarto file.

I will perform linear regressions using data from [Body Fat Data](https://hbiostat.org/data/) that is found in the book [Regression Modeling Strategies](https://hbiostat.org/rmsc/) by [Frank E Harrell Jr](https://hbiostat.org/). I will check assumptions but it is only to practice, will not adjust models based on the results/interpretation of the assumptions.

Install and load packages

```{r}
# Install and load packages

# First check if pacman is installed
if (!requireNamespace("pacman", quietly = TRUE)) {
  message("ðŸ“¦ 'pacman' not found â€” installing...")
  install.packages("pacman")
}


pacman::p_load(
  reticulate, # Allows working with Python inside R
  readr,
  here, # relative file pathways
  tidyverse, # data management and visualization
  easystats, # Easy Statistical Modeling, Visualization, and Reporting
  ggplot2
)
```

Load the Data

```{r}
# Load data
df <- read_csv(here("Input", "Data", "bodyfat.csv"))
summary(df)
```

# **R** Linear Regression

```{r}

# Violin plot age 
ggplot(df, aes(x = "", y = Age)) +
  geom_violin(fill = "darkorange", color = "black", alpha = 0.5) +
  geom_boxplot(width = 0.2, fill = "white", color = "black") +
  labs(
    title = "Age Distribution (Violin + Boxplot)",
    y = "Age",
    x = ""
  ) +
  theme_modern() 


lm_r <- lm(Height ~ Age + Wrist + Knee + Ankle, data = df)
summary(lm_r)
check_model(lm_r)

```

# **Python** Linear Regression

```{python}
#| layout-ncol: 2  # shows plots/figures in two columns 

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns


# Convert the R dataframe to a pandas DataFrame
df_py = pd.DataFrame(r.df)

# choose predictors + outcome 
X = df_py[["Age", "Wrist", "Knee","Ankle"]]
X = sm.add_constant(X)  # adds intercept
y = df_py["Height"]

lm_py = sm.OLS(y, X).fit()
print(lm_py.summary())

# Checking assumptions
fitted = lm_py.fittedvalues
residuals = lm_py.resid
plt.scatter(fitted, residuals)
plt.axhline(0, color = "red")
plt.xlabel("Fitted values")
plt.ylabel("Residuals")
plt.title("Residuals vs Fitted")
plt.show()

sns.histplot(residuals, kde = True)
plt.title("Residual Distribution")
plt.show()

sm.qqplot(residuals, line='45', fit=True)
plt.title("Qâ€“Q Plot")
plt.show()

influence = lm_py.get_influence()
cooks = influence.cooks_distance[0]

plt.stem(np.arange(len(cooks)), cooks, markerfmt=",")
plt.title("Cook's Distance")
plt.show()
```

# **SHAP** values

Here I use the **NHANES I Survival Model** to practice using **SHAP** values. The information about the example can be found in the next paragraph.

This is a cox proportional hazards model on data fromÂ [NHANES I](https://wwwn.cdc.gov/nchs/nhanes/nhanes1)Â with followup mortality data from theÂ [NHANES I Epidemiologic Followup Study](https://wwwn.cdc.gov/nchs/nhanes/nhefs). It is designed to illustrate how SHAP values enable the interpretion of XGBoost models with a clarity traditionally only provided by linear models.Â 

```{python}

import xgboost
from sklearn.model_selection import train_test_split
import shap 
plt.close('all')          # close any previous figures
plt.figure()              # start a brand new figure


X_nhanes, y_nhanes = shap.datasets.nhanesi()

plt.figure(figsize=(6, 6))

# Violin plot (orange)
sns.violinplot(
    y=X_nhanes["age"],
    color="orange",
    inner=None,      # remove internal lines so the boxplot is clear
    linewidth=1
)

# Boxplot inside the violin
sns.boxplot(
    y=X_nhanes["age"],
    width=0.2,       # thinner boxplot so the violin is still visible
    boxprops={'facecolor':'white', 'edgecolor':'black'},
    medianprops={'color':'black'},
    whiskerprops={'color':'black'},
    capprops={'color':'black'}
)

plt.title("Age Distribution (Violin + Boxplot)")
plt.ylabel("Age")
plt.xlabel("")
plt.show()



# human readable feature values
X_display, y_display = shap.datasets.nhanesi(display=True)

# Convert entire dataset to DMatrix format, so XGBoost trains faster
xgb_full = xgboost.DMatrix(X_nhanes, label=y_nhanes)

# create a train/test split
X_nhanes_train, X_nhanes_test, y_nhanes_train, y_nhanes_test = train_test_split(X_nhanes, y_nhanes, test_size=0.2, random_state=7)

# Convert train and test data to DMatrix format
xgb_train = xgboost.DMatrix(X_nhanes_train, label=y_nhanes_train)
xgb_test = xgboost.DMatrix(X_nhanes_test, label=y_nhanes_test)


# Define the hyperparameters
params = {
  "eta": 0.002, 
  "max_depth": 3, 
  "objective": "survival:cox", 
  "subsample": 0.5
  }

# Train XGBoost model
model_train = xgboost.train(
  params, 
  xgb_train, 
  10000, 
  evals=[(xgb_test, "test")], 
  verbose_eval=1000
  )
  
  
# train final model on the full data set
params = {"eta": 0.002, "max_depth": 3, "objective": "survival:cox", "subsample": 0.5}
model = xgboost.train(params, xgb_full, 5000, evals=[(xgb_full, "test")], verbose_eval=1000)

```

Check Performance

```{python}
import numpy as np

def c_statistic_harrell(pred, labels):
    # Make sure both are plain 1D NumPy arrays
    pred = np.asarray(pred).reshape(-1)
    labels = np.asarray(labels).reshape(-1)

    total = 0.0
    matches = 0.0
    n = len(labels)

    for i in range(n):
        for j in range(n):
            # In the NHANES encoding: labels > 0 = event, labels < 0 = censored
            if labels[j] > 0 and abs(labels[i]) > labels[j]:
                total += 1
                if pred[j] > pred[i]:
                    matches += 1
                elif pred[j] == pred[i]:
                    matches += 0.5   # ties count as half

    return matches / total if total > 0 else np.nan


# see how well we can order people by survival
c_statistic_harrell(model_train.predict(xgb_test), y_nhanes_test)


```


Explain the modelâ€™s predictions on the entire dataset
```{python}

plt.figure(figsize=(6, 6))
shap_values = shap.TreeExplainer(model).shap_values(X_nhanes)
shap.summary_plot(shap_values, X_nhanes)
```

